
# ‚öΩReal Time Data Warehouse Streaming

This repository contains the code and configuration for the **EURO 2024 Real-Time Data Streaming and Visualization** project. 
The project leverages a real-time data pipeline to stream football data (teams, players, matches, groups, and events) from an API into Kafka. 
The data is processed and stored in **Apache Pinot** for fast analytics and visualized using **Apache Superset**.

## üõ† Project Overview

The primary goal of this project is to demonstrate how to build and manage a **real-time data warehouse** using modern tools and technologies. 
The data includes dynamic football statistics like live match events, player performance, and group standings for EURO 2024.

### üöÄ Key Components

- **Apache Kafka**: Message broker used for real-time streaming of data.
- **Apache Pinot**: OLAP data store for fast querying and analytics.
- **Apache Superset**: Data visualization platform to create dashboards and monitor statistics.
- **Apache Airflow**: Workflow orchestration for automated data extraction and streaming.

## üìã Architecture

The project consists of the following pipeline:

1. **API Extraction**: Data is fetched from a JSON-based API.
2. **Kafka Streaming**: Extracted data is streamed into Kafka topics.
3. **Processing with Pinot**: Kafka feeds the data into Apache Pinot for storage and querying.
4. **Visualization in Superset**: Real-time dashboards are created to visualize football statistics.

## üìÇ Repository Structure

```
üì¶ Real Time Data Warehouse Streaming
‚îú‚îÄ‚îÄ dags/                    # Airflow DAGs for orchestration
‚îÇ   ‚îú‚îÄ‚îÄ euro2024_data_streaming.py
‚îú‚îÄ‚îÄ pipelines                # ETL Pipline Definition Script
‚îÇ   ‚îú‚îÄ‚îÄ euro2024_function.py
‚îÇ   ‚îú‚îÄ‚îÄ create_table_schema.py
‚îú‚îÄ‚îÄ schemas/                 # Schemas definitions 
‚îÇ   ‚îú‚îÄ‚îÄ groups_schema.json          # Schema for group data
‚îÇ   ‚îú‚îÄ‚îÄ matches_schema.json         # Schema for match data
‚îÇ   ‚îú‚îÄ‚îÄ players_schema.json         # Schema for player data
‚îÇ   ‚îî‚îÄ‚îÄ teams_schema.json           # Schema for team data
‚îú‚îÄ‚îÄ table-configs/           # Table definitions 
‚îÇ   ‚îú‚îÄ‚îÄ groups_table.json           # Table configs for group data
‚îÇ   ‚îú‚îÄ‚îÄ matches_table.json          # Table configs for match data
‚îÇ   ‚îú‚îÄ‚îÄ players_table.json          # Table configs for player data
‚îÇ   ‚îî‚îÄ‚îÄ teams_table.json            # Table configs for team data
‚îú‚îÄ‚îÄ utils/                   # Utility files (constants, helper functions)
‚îÇ   ‚îú‚îÄ‚îÄ constants.py
‚îú‚îÄ‚îÄ superset                 # Open Source Data Visualizer
‚îÇ   ‚îú‚îÄ‚îÄ dockerfile                  # Docker File for Apache Superset
‚îÇ   ‚îú‚îÄ‚îÄ superset_config.py          # Superset Image Config File
‚îÇ   ‚îî‚îÄ‚îÄ superset-init.sh            # To Initiate Bash Script For Superset  
‚îú‚îÄ‚îÄ Dockerfile               # Docker setup for containerized deployment
‚îú‚îÄ‚îÄ requirements.txt         # Python dependencies
‚îú‚îÄ‚îÄ .env
‚îú‚îÄ‚îÄ airflow.cfg
‚îî‚îÄ‚îÄ README.md                # Project documentation
```

## ‚öôÔ∏è Installation and Setup

### üõë Prerequisites

- Create a RapidAPI Account
  - Visit the [EURO 2024 API on RapidAPI](https://rapidapi.com/yuvr99-WHTEITBQbOc/api/euro-20242).
  - Sign up or log in to your RapidAPI account.
  - Subscribe to the API Gateway and obtain your API key.
- Add the RapidAPI Key
  - Navigate to the constants.py file in the utils/ directory.
  - Add your RapidAPI key in the following format:
    ```
    RAPIDAPI_KEY = "your-rapidapi-key"
    ```
- Docker and Docker Compose installed.
- Apache Kafka, Pinot, and Superset configured.
- Airflow environment set up with Python 3.10.

### Steps to Run the Project

1. **Clone the repository**:
   ```bash
   git clone https://github.com/your-username/euro-2024-kafka-pinot-pipeline.git
   cd euro-2024-kafka-pinot-pipeline
   ```
2. **Install dependencies**:
   ```bash
   pip install -r requirements.txt
   ```
3. **Build the Docker File**:
   ```bash
   docker build -t euro-2024-kafka-pinot-pipeline .
   ```
4. **Start the Docker containers**:
   ```bash
   docker-compose up
   ```
5. **Launch Airflow**:
   - Visit `http://localhost:8080` to view and manage DAGs.
  
6. **Connect Pinot Database with Superset**:
   - Visit `http://localhost:8088` in your browser.
   - Login with 'admin' as both password and user
   - Add Database > Chosse Apache Pinot as database
   - SQLAlchemy URI `pinot://pinot-broker:8099/query/sql?controller=http://pinot-controller:9000`
   - Then you can start designing the `Chart` & `Dashboards`

## üí°Usage

- **Airflow DAGs** orchestrate the extraction of data from the API and stream it to Kafka.
- **Pinot** stores the streamed data for fast querying.
- **Superset Dashboards** visualize the real-time statistics, offering insights into player performance, match events, and group standings.

## üìä Topics and Schemas

- **players**: Contains player statistics like goals, assists, and appearances.
- **teams**: Stores team details such as coach, captain, and championships.
- **matches**: Holds match details including scores, lineups, and winners.
- **groups**: Tracks group standings, points, and goal differences.

---

## üèó Architecture

Below is the high-level architecture of the real-time data streaming pipeline:

![Architecture Diagram](./images/architecture.png)

---

## Airflow DAG Pipeline

The following diagram shows the Airflow DAG for orchestrating the data pipeline:

![DAG Pipeline Diagram](./images/dag_pipeline.png)

---

## Contributing

Feel free to open issues or submit pull requests for any feature improvements or bug fixes.

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

Happy Streaming!
