
# âš½Real Time Data Warehouse Streaming

This repository contains the code and configuration for the **EURO 2024 Real-Time Data Streaming and Visualization** project. 
The project leverages a real-time data pipeline to stream football data (teams, players, matches, groups, and events) from an API into Kafka. 
The data is processed and stored in **Apache Pinot** for fast analytics and visualized using **Apache Superset**.

## ğŸ›  Project Overview

The primary goal of this project is to demonstrate how to build and manage a **real-time data warehouse** using modern tools and technologies. 
The data includes dynamic football statistics like live match events, player performance, and group standings for EURO 2024.

### ğŸš€ Key Components

- **Apache Kafka**: Message broker used for real-time streaming of data.
- **Apache Pinot**: OLAP data store for fast querying and analytics.
- **Apache Superset**: Data visualization platform to create dashboards and monitor statistics.
- **Apache Airflow**: Workflow orchestration for automated data extraction and streaming.

## ğŸ“‹ Architecture

The project consists of the following pipeline:

1. **API Extraction**: Data is fetched from a JSON-based API.
2. **Kafka Streaming**: Extracted data is streamed into Kafka topics.
3. **Processing with Pinot**: Kafka feeds the data into Apache Pinot for storage and querying.
4. **Visualization in Superset**: Real-time dashboards are created to visualize football statistics.

## ğŸ“‚ Repository Structure

```
ğŸ“¦ Real Time Data Warehouse Streaming
â”œâ”€â”€ dags/                    # Airflow DAGs for orchestration
â”‚   â”œâ”€â”€ euro2024_data_streaming.py
â”œâ”€â”€ pipelines                # ETL Pipline Definition Script
â”‚   â”œâ”€â”€ euro2024_function.py
â”‚   â”œâ”€â”€ create_table_schema.py
â”œâ”€â”€ schemas/                 # Schemas definitions 
â”‚   â”œâ”€â”€ groups_schema.json          # Schema for group data
â”‚   â”œâ”€â”€ matches_schema.json         # Schema for match data
â”‚   â”œâ”€â”€ players_schema.json         # Schema for player data
â”‚   â””â”€â”€ teams_schema.json           # Schema for team data
â”œâ”€â”€ table-configs/           # Table definitions 
â”‚   â”œâ”€â”€ groups_table.json           # Table configs for group data
â”‚   â”œâ”€â”€ matches_table.json          # Table configs for match data
â”‚   â”œâ”€â”€ players_table.json          # Table configs for player data
â”‚   â””â”€â”€ teams_table.json            # Table configs for team data
â”œâ”€â”€ utils/                   # Utility files (constants, helper functions)
â”‚   â”œâ”€â”€ constants.py
â”œâ”€â”€ superset                 # Open Source Data Visualizer
â”‚   â”œâ”€â”€ dockerfile                  # Docker File for Apache Superset
â”‚   â”œâ”€â”€ superset_config.py          # Superset Image Config File
â”‚   â””â”€â”€ superset-init.sh            # To Initiate Bash Script For Superset  
â”œâ”€â”€ Dockerfile               # Docker setup for containerized deployment
â”œâ”€â”€ requirements.txt         # Python dependencies
â”œâ”€â”€ .env
â”œâ”€â”€ airflow.cfg
â””â”€â”€ README.md                # Project documentation
```

## âš™ï¸ Installation and Setup

### ğŸ›‘ Prerequisites

- Create a RapidAPI Account
  - Visit the [EURO 2024 API on RapidAPI](https://rapidapi.com/yuvr99-WHTEITBQbOc/api/euro-20242).
  - Sign up or log in to your RapidAPI account.
  - Subscribe to the API Gateway and obtain your API key.
- Add the RapidAPI Key
  - Navigate to the constants.py file in the utils/ directory.
  - Add your RapidAPI key in the following format:
    ```
    RAPIDAPI_KEY = "your-rapidapi-key"
    ```
- Docker and Docker Compose installed.
- Apache Kafka, Pinot, and Superset configured.
- Airflow environment set up with Python 3.10.

### Steps to Run the Project

1. **Clone the repository**:
   ```bash
   git clone https://github.com/your-username/euro-2024-kafka-pinot-pipeline.git
   cd euro-2024-kafka-pinot-pipeline
   ```
2. **Install dependencies**:
   ```bash
   pip install -r requirements.txt
   ```
3. **Build the Docker File**:
   ```bash
   docker build -t euro-2024-kafka-pinot-pipeline .
   ```
4. **Start the Docker containers**:
   ```bash
   docker-compose up
   ```
5. **Access Superset**:
   - Visit `http://localhost:8088` in your browser.

6. **Launch Airflow**:
   - Visit `http://localhost:8080` to view and manage DAGs.

## ğŸ’¡Usage

- **Airflow DAGs** orchestrate the extraction of data from the API and stream it to Kafka.
- **Pinot** stores the streamed data for fast querying.
- **Superset Dashboards** visualize the real-time statistics, offering insights into player performance, match events, and group standings.

## ğŸ“Š Topics and Schemas

- **players**: Contains player statistics like goals, assists, and appearances.
- **teams**: Stores team details such as coach, captain, and championships.
- **matches**: Holds match details including scores, lineups, and winners.
- **groups**: Tracks group standings, points, and goal differences.

---

## ğŸ— Architecture

Below is the high-level architecture of the real-time data streaming pipeline:

![Architecture Diagram](./images/architecture.png)

---

## Airflow DAG Pipeline

The following diagram shows the Airflow DAG for orchestrating the data pipeline:

![DAG Pipeline Diagram](./images/dag_pipeline.png)

---

## Contributing

Feel free to open issues or submit pull requests for any feature improvements or bug fixes.

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

Happy Streaming!
